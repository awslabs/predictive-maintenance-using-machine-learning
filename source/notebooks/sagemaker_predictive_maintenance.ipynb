{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import urllib\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Data gathering, storing, denoising, alignment, curating and querying is THE most complicated aspect of predictive maintenance. Fault reports can for example be buried in pdf scans, fault types being misdiagnosed, fault only discovered weeks after they actually occured, or reported in a different timezone without the timezone information. These are a few of a thousands of pitfalls that may\n",
    "await you when trying to collate such real-world dataset at scale. In this scenario we are using a hypothetical dataset where the data is cleaned and aligned.\n",
    "\n",
    "\n",
    "### Background\n",
    "NASAâ€™s Prognostic Center of Excellence established a repository with datasets to be used for benchmarking prognostics and predictive maintenance related algorithms. Among these datasets involves data from a turbofan engine simulation model C-MAPPS (or Commercial Modular Aero Propulsion System Simulation). The references section contains details about the over 100 publications using this dataset. C-MAPPS is a tool used to generate health, control and engine parameters from a simulated turbofan engine. A custom code wrapper was used to inject synthetic faults and continuous degradation trends into a time series of sensor data. Some high level characteristics of this dataset are as follows:\n",
    "The data obtained is from a high fidelity simulation of a turbofan engine, but closely models the sensor values of an actual engine. Synthetic noise was added to the dataset to replicate real-world scenarios. The effects of faults are masked due to operational conditions, which is a common trait of most real world systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "urllib.request.urlretrieve('https://ti.arc.nasa.gov/m/project/prognostic-repository/CMAPSSData.zip', os.path.join(data_folder, 'CMAPSSData.zip'))\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(data_folder, 'CMAPSSData.zip'), \"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_folder)\n",
    "    \n",
    "columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3','s4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14','s15', 's16', 's17', 's18', 's19', 's20', 's21']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "There are 4 different dataset and we will train our models on each individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize sensor readings\n",
    "train_df = []\n",
    "eps = 0.000001 # for floating point issues during normalization \n",
    "for i in range(1,5):\n",
    "    df = pd.read_csv('data/train_FD{:03d}.txt'.format(i), delimiter=' ', header=None)\n",
    "    df.drop(df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    df.columns = columns\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    train_df.append(df)\n",
    "\n",
    "# compute RUL (remaining useful life)\n",
    "for i, df in enumerate(train_df):\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left')\n",
    "    df['RUL'] = df['max'] - df['cycle']\n",
    "    df.drop('max', axis=1, inplace=True)\n",
    "    train_df[i]=df\n",
    "\n",
    "train_df[0].head()\n",
    "o = train_df[0][columns[2:10]][train_df[0]['id'] == 3].plot(subplots=True, sharex=True, figsize=(20,10), title=\"Train: 8 sensors of Engine 1 before failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "We read at the same time the testing data, and the actual RUL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "for i in range(1,5):\n",
    "    # Load time series\n",
    "    df = pd.read_csv('data/test_FD{:03d}.txt'.format(i), delimiter=' ', header=None)\n",
    "    df.drop(df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    \n",
    "    # Load the RUL values\n",
    "    df_rul = pd.read_csv('data/RUL_FD{:03d}.txt'.format(i), delimiter=' ', header=None)    \n",
    "    df_rul.drop(df_rul.columns[1], axis=1, inplace=True)\n",
    "    df_rul.index += 1\n",
    "    \n",
    "    # Merge RUL and timeseries and compute RUL per timestamp\n",
    "    df = df.merge(df_rul, left_on=df.columns[0], right_index=True, how='left')\n",
    "    df.columns = columns + ['RUL_end']\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left') # We get the number of cycles per series\n",
    "    df['RUL'] = df['max'] + df['RUL_end'] - df['cycle'] # The RUL is the number of cycles per series + RUL - how many cycles have already ran\n",
    "    df.drop(['max','RUL_end'], axis=1, inplace=True)\n",
    "    \n",
    "    # Normalize\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    test_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker MXNet Estimator\n",
    "\n",
    "Now we will go over the steps needed to define the MXNet model and train with SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload processed data to S3 for training\n",
    "\n",
    "We have to upload the processed data to a location in S3 so that the SageMaker training instance can access the data from that location. We will also, at the same time, upload the test data to the S3 bucket so that we can use that as an input to the trained model for scheduled inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket = 'sagemaker-predictive-maintenance'\n",
    "prefix = 'pred-maintenance-artifacts'\n",
    "\n",
    "s3_bucket_resource = boto3.resource('s3').Bucket(bucket)\n",
    "\n",
    "# Upload raw data files to S3\n",
    "for subdir, dirs, files in os.walk(data_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        s3_path = os.path.join(prefix, full_path)\n",
    "        s3_bucket_resource.Object(s3_path).upload_file(full_path)\n",
    "\n",
    "# Upload processed test data for inference\n",
    "for i in range(len(test_df)):\n",
    "    local_test_file = 'data/test-{}.csv'.format(i)\n",
    "    test_df[i].to_csv(local_test_file)\n",
    "    s3_test_file = os.path.join(prefix, 'data', 'test-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_test_file).upload_file(local_test_file)\n",
    "\n",
    "# Upload processed data for training\n",
    "for i in range(len(train_df)):\n",
    "    local_train_file = 'data/train-{}.csv'.format(i)\n",
    "    train_df[i].to_csv(local_train_file)\n",
    "    s3_train_file = os.path.join(prefix, 'train', 'train-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_train_file).upload_file(local_train_file)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train')\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model output location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MXNet Model Training script\n",
    "\n",
    "Training MXNet models using MXNet Estimators is a two-step process. First, you prepare your training script, then second, you run this on SageMaker via an MXNet Estimator. The training script we have prepared for the model is located in the `sagemaker_predictive_maintenance_entry_point` folder. \n",
    "\n",
    "The training script contains functions to create the model for training and for inference. We also have functions to convert our dataframes into a Gluon Dataset so that it can be efficiently prefetched, transformed into numerical features used by the network and padded so that we can learn from multiple samples in batches.\n",
    "\n",
    "For more information on how to setup a training script for SageMaker using the MXNet estimator see: https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#preparing-the-mxnet-training-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize sagemaker_predictive_maintenance_entry_point/sagemaker_predictive_maintenance_entry_point.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MXNet Estimator\n",
    "\n",
    "Now, we can start the SageMaker training job by creating an MXNet estimator. We pass in the required arguments such as the `entry_point`, `role`, `train_instance_type`, and `train_instance_count` into the MXNet Estimator constructor.\n",
    "\n",
    "Then we start the training script by calling `fit` on the MXNet Estimator. `fit` takes both required and optional arguments. The required argument here is the S3 location of the training data passed in as a dictionary. We are also adding an optional argument for the job name. This is important because when the training job is complete and SageMaker needs to create a SageMaker Model for real-time inference or batch transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "training_job_name = 'pred-maintenance-mxnet-model'\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "m = MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n",
    "          source_dir='sagemaker_predictive_maintenance_entry_point',\n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=train_instance_type,\n",
    "          output_path=output_location,\n",
    "          hyperparameters={'num-datasets' : len(train_df),\n",
    "                           'num-gpus': 1,\n",
    "                           'epochs': 500,\n",
    "                           'optimizer': 'adam',\n",
    "                           'batch-size':1,\n",
    "                           'log-interval': 100},\n",
    "         input_mode='File',\n",
    "         train_max_run=7200,\n",
    "         framework_version='1.6.0')\n",
    "\n",
    "m.fit({'train': s3_train_data}, job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Transformer Model\n",
    "\n",
    "We can now call the `transformer` function to create a SageMaker Model with the trained model. The SageMaker Model will have the same name as the training job that just completed. This will ensure that SageMaker stores a reference to the trained model which can be used for predictions later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/{}'.format(bucket, prefix, 'batch-inference')\n",
    "transformer = m.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform test data using the transformer model\n",
    "\n",
    "Using the `transformer` SageMaker Model, we can run a [SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) job to get some predictions on test dataset for the model. Here we have a function that takes some test data in S3 and copies it to a new location where it's used as the input to the `transform` function of the Batch Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_test_key = \"pred-maintenance-artifacts/data/test-0.csv\"\n",
    "s3_transform_input = os.path.join(prefix, \"batch-transform-input\")\n",
    "\n",
    "def get_transform_input():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=s3_test_key)\n",
    "    test_file = s3_response[\"Body\"].read()\n",
    "\n",
    "    test_df_entry = pd.read_csv(io.BytesIO(test_file))\n",
    "    test_data = test_df_entry[test_df_entry['id']==0+1][test_df_entry.columns[2:-1]].values\n",
    "    test_data = test_data[0:test_data.shape[0]-1,:].astype('float32')\n",
    "    data_payload = {'input':np.expand_dims(test_data, axis=0).tolist()}\n",
    "    \n",
    "    job_name = 'predictive-maintenance-batch-transform-job-{}'.format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "    s3_batch_transform_input_key = os.path.join(s3_transform_input, job_name)\n",
    "    \n",
    "    s3_client.put_object(Body=json.dumps(data_payload),\n",
    "                         Bucket=bucket, \n",
    "                         Key=s3_batch_transform_input_key)\n",
    "    return job_name, 's3://{}/{}'.format(bucket, s3_batch_transform_input_key)\n",
    "\n",
    "job_name, input_key = get_transform_input()\n",
    "transformer.transform(input_key, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model prediction results\n",
    "\n",
    "Once the transform job terminates, we can see the models predictions for the fractional remaining useful life left for the sensor readings in the `data/test-0.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_output():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=os.path.join(prefix, \n",
    "                                                                       'batch-inference', \n",
    "                                                                       job_name+'.out'))\n",
    "    transform_out = np.array(eval(s3_response[\"Body\"].read()))\n",
    "    return transform_out\n",
    "    \n",
    "get_transform_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
